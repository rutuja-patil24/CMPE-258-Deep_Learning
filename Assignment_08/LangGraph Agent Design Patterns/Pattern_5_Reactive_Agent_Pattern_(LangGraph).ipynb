{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyVjo/GykBlLMFkd9S/WFr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rutuja-patil24/CMPE-258-Deep_Learning/blob/main/Assignment_08/LangGraph%20Agent%20Design%20Patterns/Pattern_5_Reactive_Agent_Pattern_(LangGraph).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Install Required Packages"
      ],
      "metadata": {
        "id": "tYnzTipXZ03k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui6RZFJ-TC6G",
        "outputId": "ee7a0696-3953-4b7b-eaa1-76c615012d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langgraph langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF3qoHo4T7Kv",
        "outputId": "eb0a6f9a-468e-48ee-abfb-a087be6f1976"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m2.4/2.5 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Modules"
      ],
      "metadata": {
        "id": "r3gNQBrIZvxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "#  Prompt user to enter keys securely (not visible in UI)\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Reactive_Agent\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbfwRtdLTh0N",
        "outputId": "6019fcfe-9223-42fc-b95b-250b56865132"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n",
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3: Define the LLM"
      ],
      "metadata": {
        "id": "Cn0jbCFUZsa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o-19ArdTVxr",
        "outputId": "c4e0de53-a6f3-4930-d467-3341f2ef174b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e7847f6b8c56>:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define the State"
      ],
      "metadata": {
        "id": "xjnNjPe4ZrFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "\n",
        "class FAQState(TypedDict):\n",
        "    question: str\n",
        "    answer: str"
      ],
      "metadata": {
        "id": "NsJKCNtwUQuP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Define the Node (Reactive Response"
      ],
      "metadata": {
        "id": "BS_WnvpSZntG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "faq_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a helpful support agent. Answer this customer FAQ:\n",
        "Question: {question}\n",
        "\n",
        "If it's unknown, respond with \"Let me escalate that to our human team.\"\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "faq_node = faq_prompt | llm\n"
      ],
      "metadata": {
        "id": "j9CFy_tYUSlq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Define Transition Function"
      ],
      "metadata": {
        "id": "cYzGm6YpZj1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_answer(state: FAQState) -> FAQState:\n",
        "    response = faq_node.invoke({\"question\": state[\"question\"]})\n",
        "    return {\"question\": state[\"question\"], \"answer\": response.content}"
      ],
      "metadata": {
        "id": "iBECyuAWZPOM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 7: Build the Graph"
      ],
      "metadata": {
        "id": "VKB2_o-6ZeI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(FAQState)\n",
        "builder.add_node(\"faq_response\", RunnableLambda(update_answer))\n",
        "builder.set_entry_point(\"faq_response\")\n",
        "builder.set_finish_point(\"faq_response\")\n",
        "\n",
        "faq_graph = builder.compile()\n"
      ],
      "metadata": {
        "id": "j1dhysrPZR_P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Run the Graph"
      ],
      "metadata": {
        "id": "fY56hJwGZbI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = faq_graph.invoke({\"question\": \"How long does shipping take?\"})\n",
        "print(\"Response:\\n\", result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DodGipA_ZUO_",
        "outputId": "22df636e-9534-4258-e931-97060e9d22ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            " Our standard shipping time is typically 3-5 business days. However, please keep in mind that shipping times may vary depending on your location and any potential delays with the carrier. If you have any concerns about the delivery timeframe, feel free to reach out to us for more specific information.\n"
          ]
        }
      ]
    }
  ]
}