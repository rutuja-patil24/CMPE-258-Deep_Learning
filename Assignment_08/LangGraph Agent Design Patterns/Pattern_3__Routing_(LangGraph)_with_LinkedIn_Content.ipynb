{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSYvmWLBYEDsrBgZsM6gEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rutuja-patil24/CMPE-258-Deep_Learning/blob/main/Assignment_08/LangGraph%20Agent%20Design%20Patterns/Pattern_3__Routing_(LangGraph)_with_LinkedIn_Content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install dependencies"
      ],
      "metadata": {
        "id": "IFEpktUwnFYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langgraph langchain-openai openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8NkxXr4nJJz",
        "outputId": "0313849d-0f33-491c-a296-7dbb07ddeecc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.9/148.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langsmith import traceable"
      ],
      "metadata": {
        "id": "NtYPApMwog4d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Set Environment for LangSmith Tracing"
      ],
      "metadata": {
        "id": "Pe4T7LA4nLku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF4XEq-7efnV",
        "outputId": "8795b8c3-f59f-4ea5-bc0f-87dc10e500db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n",
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "#  Prompt user to enter keys securely (not visible in UI)\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Routing\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3: Initialize LLM"
      ],
      "metadata": {
        "id": "EKeIKv1AoBaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-community\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "sLNbakgLXMiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72afcb03-76d2-4b82-e44e-9b2cab4c5226"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/2.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-1e9109a9b616>:4: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal\n",
        "\n",
        "class RoutingState(TypedDict):\n",
        "    request: str\n",
        "    decision: Literal[\"tip\", \"announcement\", \"post\"]\n",
        "    output: str\n"
      ],
      "metadata": {
        "id": "_aRJPq18XgIE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define the Router Node"
      ],
      "metadata": {
        "id": "V-JPcOB0o_z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route_marketing_request(state: RoutingState) -> dict:\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"Classify this LinkedIn content request as 'tip', 'announcement', or 'post':\n",
        "\n",
        "Request: {request}\n",
        "\n",
        "Respond with only one word.\"\"\"\n",
        "    )\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"request\": state[\"request\"]}).content.lower().strip()\n",
        "    return {\"decision\": result}\n"
      ],
      "metadata": {
        "id": "3HKZ9VA4XSui"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Define Writer Nodes"
      ],
      "metadata": {
        "id": "gpnOU63xpFNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tip_writer(state: RoutingState):\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Write 3 bullet point tips for LinkedIn based on this topic: {request}\"\n",
        "    )\n",
        "    return {\"output\": (prompt | llm).invoke({\"request\": state[\"request\"]}).content}\n",
        "\n",
        "def announcer(state: RoutingState):\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Write a short, professional LinkedIn announcement about this: {request}\"\n",
        "    )\n",
        "    return {\"output\": (prompt | llm).invoke({\"request\": state[\"request\"]}).content}\n",
        "\n",
        "def post_writer(state: RoutingState):\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Write a compelling LinkedIn post about: {request}\"\n",
        "    )\n",
        "    return {\"output\": (prompt | llm).invoke({\"request\": state[\"request\"]}).content}\n"
      ],
      "metadata": {
        "id": "66rUMULZXkb5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Build the LangGraph"
      ],
      "metadata": {
        "id": "z2TI2bCepLKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(RoutingState)\n",
        "\n",
        "# Add all nodes\n",
        "builder.add_node(\"router\", route_marketing_request)\n",
        "builder.add_node(\"tip_node\", tip_writer)\n",
        "builder.add_node(\"announcement_node\", announcer)\n",
        "builder.add_node(\"post_node\", post_writer)\n",
        "\n",
        "# Entry point\n",
        "builder.set_entry_point(\"router\")\n",
        "\n",
        "# Add conditional routing logic\n",
        "builder.add_conditional_edges(\"router\", lambda state: state[\"decision\"], {\n",
        "    \"tip\": \"tip_node\",\n",
        "    \"announcement\": \"announcement_node\",\n",
        "    \"post\": \"post_node\"\n",
        "})\n",
        "\n",
        "# All writers lead to END\n",
        "builder.add_edge(\"tip_node\", END)\n",
        "builder.add_edge(\"announcement_node\", END)\n",
        "builder.add_edge(\"post_node\", END)\n",
        "\n",
        "# Compile workflow\n",
        "workflow = builder.compile()\n"
      ],
      "metadata": {
        "id": "ugSXWAQZXqaN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Try with a Real Example"
      ],
      "metadata": {
        "id": "-cYKrpeepQG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "0CLRruyxp-Pv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"request\": \"Share 3 practical tips for using ChatGPT to improve productivity at work\"}\n",
        "\n",
        "final_state = workflow.invoke(inputs)\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(final_state)\n"
      ],
      "metadata": {
        "id": "IvCJdfvxXt07",
        "outputId": "977f8168-6822-44a9-f63c-45aa5539a8ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'decision': 'tip',\n",
            " 'output': '- Use ChatGPT to quickly generate ideas or solutions for '\n",
            "           'work-related tasks or projects, saving time and boosting '\n",
            "           'productivity.\\n'\n",
            "           '- Utilize ChatGPT to streamline communication with colleagues or '\n",
            "           'clients by generating clear and concise messages or responses.\\n'\n",
            "           '- Leverage ChatGPT to automate repetitive tasks or processes, '\n",
            "           'freeing up time to focus on more strategic or high-priority work.',\n",
            " 'request': 'Share 3 practical tips for using ChatGPT to improve productivity '\n",
            "            'at work'}\n"
          ]
        }
      ]
    }
  ]
}